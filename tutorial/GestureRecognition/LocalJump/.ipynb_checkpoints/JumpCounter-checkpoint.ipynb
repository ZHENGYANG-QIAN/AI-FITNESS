{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p1lN9P53zTuf"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "def show_image(img, figsize=(10, 10)):\n",
    "  \"\"\"Shows output PIL image.\"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QBrKOeP30RAx"
   },
   "outputs": [],
   "source": [
    "class FullBodyPoseEmbedder(object):\n",
    "  \"\"\"Converts 3D pose landmarks into 3D embedding.\"\"\"\n",
    "\n",
    "  def __init__(self, torso_size_multiplier=2.5):\n",
    "    # Multiplier to apply to the torso to get minimal body size.\n",
    "    self._torso_size_multiplier = torso_size_multiplier\n",
    "\n",
    "    # Names of the landmarks as they appear in the prediction.\n",
    "    self._landmark_names = [\n",
    "        'nose',\n",
    "        'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "        'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "        'left_ear', 'right_ear',\n",
    "        'mouth_left', 'mouth_right',\n",
    "        'left_shoulder', 'right_shoulder',\n",
    "        'left_elbow', 'right_elbow',\n",
    "        'left_wrist', 'right_wrist',\n",
    "        'left_pinky_1', 'right_pinky_1',\n",
    "        'left_index_1', 'right_index_1',\n",
    "        'left_thumb_2', 'right_thumb_2',\n",
    "        'left_hip', 'right_hip',\n",
    "        'left_knee', 'right_knee',\n",
    "        'left_ankle', 'right_ankle',\n",
    "        'left_heel', 'right_heel',\n",
    "        'left_foot_index', 'right_foot_index',\n",
    "    ]\n",
    "\n",
    "  def __call__(self, landmarks):\n",
    "    \"\"\"Normalizes pose landmarks and converts to embedding\n",
    "    \n",
    "    Args:\n",
    "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Result:\n",
    "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
    "      pairwise distances defined in `_get_pose_distance_embedding`.\n",
    "    \"\"\"\n",
    "    assert landmarks.shape[0] == len(self._landmark_names), 'Unexpected number of landmarks: {}'.format(landmarks.shape[0])\n",
    "\n",
    "    # Get pose landmarks.\n",
    "    landmarks = np.copy(landmarks)\n",
    "\n",
    "    # Normalize landmarks.\n",
    "    landmarks = self._normalize_pose_landmarks(landmarks)\n",
    "\n",
    "    # Get embedding.\n",
    "    embedding = self._get_pose_distance_embedding(landmarks)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "  def _normalize_pose_landmarks(self, landmarks):\n",
    "    \"\"\"Normalizes landmarks translation and scale.\"\"\"\n",
    "    landmarks = np.copy(landmarks)\n",
    "\n",
    "    # Normalize translation.\n",
    "    pose_center = self._get_pose_center(landmarks)\n",
    "    landmarks -= pose_center\n",
    "\n",
    "    # Normalize scale.\n",
    "    pose_size = self._get_pose_size(landmarks, self._torso_size_multiplier)\n",
    "    landmarks /= pose_size\n",
    "    # Multiplication by 100 is not required, but makes it eaasier to debug.\n",
    "    landmarks *= 100\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "  def _get_pose_center(self, landmarks):\n",
    "    \"\"\"Calculates pose center as point between hips.\"\"\"\n",
    "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
    "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
    "    center = (left_hip + right_hip) * 0.5\n",
    "    return center\n",
    "\n",
    "  def _get_pose_size(self, landmarks, torso_size_multiplier):\n",
    "    \"\"\"Calculates pose size.\n",
    "    \n",
    "    It is the maximum of two values:\n",
    "      * Torso size multiplied by `torso_size_multiplier`\n",
    "      * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # This approach uses only 2D landmarks to compute pose size.\n",
    "    landmarks = landmarks[:, :2]\n",
    "\n",
    "    # Hips center.\n",
    "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
    "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
    "    hips = (left_hip + right_hip) * 0.5\n",
    "\n",
    "    # Shoulders center.\n",
    "    left_shoulder = landmarks[self._landmark_names.index('left_shoulder')]\n",
    "    right_shoulder = landmarks[self._landmark_names.index('right_shoulder')]\n",
    "    shoulders = (left_shoulder + right_shoulder) * 0.5\n",
    "\n",
    "    # Torso size as the minimum body size.\n",
    "    torso_size = np.linalg.norm(shoulders - hips)\n",
    "\n",
    "    # Max dist to pose center.\n",
    "    pose_center = self._get_pose_center(landmarks)\n",
    "    max_dist = np.max(np.linalg.norm(landmarks - pose_center, axis=1))\n",
    "\n",
    "    return max(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "  def _get_pose_distance_embedding(self, landmarks):\n",
    "    \"\"\"Converts pose landmarks into 3D embedding.\n",
    "\n",
    "    We use several pairwise 3D distances to form pose embedding. All distances\n",
    "    include X and Y components with sign. We differnt types of pairs to cover\n",
    "    different pose classes. Feel free to remove some or add new.\n",
    "    \n",
    "    Args:\n",
    "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Result:\n",
    "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
    "      pairwise distances.\n",
    "    \"\"\"\n",
    "    embedding = np.array([\n",
    "        # One joint.\n",
    "\n",
    "        self._get_distance(\n",
    "            self._get_average_by_names(landmarks, 'left_hip', 'right_hip'),\n",
    "            self._get_average_by_names(landmarks, 'left_shoulder', 'right_shoulder')),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_elbow'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_elbow'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_elbow', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_elbow', 'right_wrist'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_knee'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_knee'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_knee', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_knee', 'right_ankle'),\n",
    "\n",
    "        # Two joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_wrist'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_ankle'),\n",
    "\n",
    "        # Four joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
    "\n",
    "        # Five joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_ankle'),\n",
    "        \n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
    "\n",
    "        # Cross body.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_elbow', 'right_elbow'),\n",
    "        self._get_distance_by_names(landmarks, 'left_knee', 'right_knee'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_wrist', 'right_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'left_ankle', 'right_ankle'),\n",
    "\n",
    "        # Body bent direction.\n",
    "\n",
    "        # self._get_distance(\n",
    "        #     self._get_average_by_names(landmarks, 'left_wrist', 'left_ankle'),\n",
    "        #     landmarks[self._landmark_names.index('left_hip')]),\n",
    "        # self._get_distance(\n",
    "        #     self._get_average_by_names(landmarks, 'right_wrist', 'right_ankle'),\n",
    "        #     landmarks[self._landmark_names.index('right_hip')]),\n",
    "    ])\n",
    "\n",
    "    return embedding\n",
    "\n",
    "  def _get_average_by_names(self, landmarks, name_from, name_to):\n",
    "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
    "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
    "    return (lmk_from + lmk_to) * 0.5\n",
    "\n",
    "  def _get_distance_by_names(self, landmarks, name_from, name_to):\n",
    "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
    "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
    "    return self._get_distance(lmk_from, lmk_to)\n",
    "\n",
    "  def _get_distance(self, lmk_from, lmk_to):\n",
    "    return lmk_to - lmk_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678795163548,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "TR9M0uAK1WzX"
   },
   "outputs": [],
   "source": [
    "class PoseSample(object):\n",
    "\n",
    "  def __init__(self, name, landmarks, class_name, embedding):\n",
    "    self.name = name\n",
    "    self.landmarks = landmarks\n",
    "    self.class_name = class_name\n",
    "    \n",
    "    self.embedding = embedding\n",
    "\n",
    "\n",
    "class PoseSampleOutlier(object):\n",
    "\n",
    "  def __init__(self, sample, detected_class, all_classes):\n",
    "    self.sample = sample\n",
    "    self.detected_class = detected_class\n",
    "    self.all_classes = all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1678795166466,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "y230jVvP1u33"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class PoseClassifier(object):\n",
    "  \"\"\"Classifies pose landmarks.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               pose_samples_folder,\n",
    "               pose_embedder,\n",
    "               file_extension='csv',\n",
    "               file_separator=',',\n",
    "               n_landmarks=33,\n",
    "               n_dimensions=3,\n",
    "               top_n_by_max_distance=30,\n",
    "               top_n_by_mean_distance=10,\n",
    "               axes_weights=(1., 1., 0.2)):\n",
    "    self._pose_embedder = pose_embedder\n",
    "    self._n_landmarks = n_landmarks\n",
    "    self._n_dimensions = n_dimensions\n",
    "    self._top_n_by_max_distance = top_n_by_max_distance\n",
    "    self._top_n_by_mean_distance = top_n_by_mean_distance\n",
    "    self._axes_weights = axes_weights\n",
    "\n",
    "    self._pose_samples = self._load_pose_samples(pose_samples_folder,\n",
    "                                                 file_extension,\n",
    "                                                 file_separator,\n",
    "                                                 n_landmarks,\n",
    "                                                 n_dimensions,\n",
    "                                                 pose_embedder)\n",
    "\n",
    "  def _load_pose_samples(self,\n",
    "                         pose_samples_folder,\n",
    "                         file_extension,\n",
    "                         file_separator,\n",
    "                         n_landmarks,\n",
    "                         n_dimensions,\n",
    "                         pose_embedder):\n",
    "    \"\"\"Loads pose samples from a given folder.\n",
    "    \n",
    "    Required folder structure:\n",
    "      neutral_standing.csv\n",
    "      pushups_open.csv\n",
    "      pushups_close.csv\n",
    "      jumps_close.csv\n",
    "      ...\n",
    "\n",
    "    Required CSV structure:\n",
    "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
    "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
    "      ...\n",
    "    \"\"\"\n",
    "    # Each file in the folder represents one pose class.\n",
    "    file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
    "\n",
    "    pose_samples = []\n",
    "    for file_name in file_names:\n",
    "      # Use file name as pose class name.\n",
    "      class_name = file_name[:-(len(file_extension) + 1)]\n",
    "      \n",
    "      # Parse CSV.\n",
    "      with open(os.path.join(pose_samples_folder, file_name)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=file_separator)\n",
    "        for row in csv_reader:\n",
    "          assert len(row) == n_landmarks * n_dimensions + 1, 'Wrong number of values: {}'.format(len(row))\n",
    "          landmarks = np.array(row[1:], np.float32).reshape([n_landmarks, n_dimensions])\n",
    "          pose_samples.append(PoseSample(\n",
    "              name=row[0],\n",
    "              landmarks=landmarks,\n",
    "              class_name=class_name,\n",
    "              embedding=pose_embedder(landmarks),\n",
    "          ))\n",
    "\n",
    "    return pose_samples\n",
    "\n",
    "  def find_pose_sample_outliers(self):\n",
    "    \"\"\"Classifies each sample against the entire database.\"\"\"\n",
    "    # Find outliers in target poses\n",
    "    outliers = []\n",
    "    for sample in self._pose_samples:\n",
    "      # Find nearest poses for the target one.\n",
    "      pose_landmarks = sample.landmarks.copy()\n",
    "      pose_classification = self.__call__(pose_landmarks)\n",
    "      class_names = [class_name for class_name, count in pose_classification.items() if count == max(pose_classification.values())]\n",
    "\n",
    "      # Sample is an outlier if nearest poses have different class or more than\n",
    "      # one pose class is detected as nearest.\n",
    "      if sample.class_name not in class_names or len(class_names) != 1:\n",
    "        outliers.append(PoseSampleOutlier(sample, class_names, pose_classification))\n",
    "\n",
    "    return outliers\n",
    "\n",
    "  def __call__(self, pose_landmarks):\n",
    "    \"\"\"Classifies given pose.\n",
    "\n",
    "    Classification is done in two stages:\n",
    "      * First we pick top-N samples by MAX distance. It allows to remove samples\n",
    "        that are almost the same as given pose, but has few joints bent in the\n",
    "        other direction.\n",
    "      * Then we pick top-N samples by MEAN distance. After outliers are removed\n",
    "        on a previous step, we can pick samples that are closes on average.\n",
    "    \n",
    "    Args:\n",
    "      pose_landmarks: NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with count of nearest pose samples from the database. Sample:\n",
    "        {\n",
    "          'pushups_down': 8,\n",
    "          'pushups_up': 2,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Check that provided and target poses have the same shape.\n",
    "    assert pose_landmarks.shape == (self._n_landmarks, self._n_dimensions), 'Unexpected shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "    # Get given pose embedding.\n",
    "    pose_embedding = self._pose_embedder(pose_landmarks)\n",
    "    flipped_pose_embedding = self._pose_embedder(pose_landmarks * np.array([-1, 1, 1]))\n",
    "\n",
    "    # Filter by max distance.\n",
    "    #\n",
    "    # That helps to remove outliers - poses that are almost the same as the\n",
    "    # given one, but has one joint bent into another direction and actually\n",
    "    # represnt a different pose class.\n",
    "    max_dist_heap = []\n",
    "    for sample_idx, sample in enumerate(self._pose_samples):\n",
    "      max_dist = min(\n",
    "          np.max(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.max(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      max_dist_heap.append([max_dist, sample_idx])\n",
    "\n",
    "    max_dist_heap = sorted(max_dist_heap, key=lambda x: x[0])\n",
    "    max_dist_heap = max_dist_heap[:self._top_n_by_max_distance]\n",
    "\n",
    "    # Filter by mean distance.\n",
    "    #\n",
    "    # After removing outliers we can find the nearest pose by mean distance.\n",
    "    mean_dist_heap = []\n",
    "    for _, sample_idx in max_dist_heap:\n",
    "      sample = self._pose_samples[sample_idx]\n",
    "      mean_dist = min(\n",
    "          np.mean(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.mean(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      mean_dist_heap.append([mean_dist, sample_idx])\n",
    "\n",
    "    mean_dist_heap = sorted(mean_dist_heap, key=lambda x: x[0])\n",
    "    mean_dist_heap = mean_dist_heap[:self._top_n_by_mean_distance]\n",
    "\n",
    "    # Collect results into map: (class_name -> n_samples)\n",
    "    class_names = [self._pose_samples[sample_idx].class_name for _, sample_idx in mean_dist_heap]\n",
    "    result = {class_name: class_names.count(class_name) for class_name in set(class_names)}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1678795172533,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "POC4_eQsE3VO"
   },
   "outputs": [],
   "source": [
    "class EMADictSmoothing(object):\n",
    "  \"\"\"Smoothes pose classification.\"\"\"\n",
    "\n",
    "  def __init__(self, window_size=10, alpha=0.2):\n",
    "    self._window_size = window_size\n",
    "    self._alpha = alpha\n",
    "\n",
    "    self._data_in_window = []\n",
    "\n",
    "  def __call__(self, data):\n",
    "    \"\"\"Smoothes given pose classification.\n",
    "\n",
    "    Smoothing is done by computing Exponential Moving Average for every pose\n",
    "    class observed in the given time window. Missed pose classes arre replaced\n",
    "    with 0.\n",
    "    \n",
    "    Args:\n",
    "      data: Dictionary with pose classification. Sample:\n",
    "          {\n",
    "            'pushups_down': 8,\n",
    "            'pushups_up': 2,\n",
    "          }\n",
    "\n",
    "    Result:\n",
    "      Dictionary in the same format but with smoothed and float instead of\n",
    "      integer values. Sample:\n",
    "        {\n",
    "          'pushups_down': 8.3,\n",
    "          'pushups_up': 1.7,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Add new data to the beginning of the window for simpler code.\n",
    "    self._data_in_window.insert(0, data)\n",
    "    self._data_in_window = self._data_in_window[:self._window_size]\n",
    "\n",
    "    # Get all keys.\n",
    "    keys = set([key for data in self._data_in_window for key, _ in data.items()])\n",
    "\n",
    "    # Get smoothed values.\n",
    "    smoothed_data = dict()\n",
    "    for key in keys:\n",
    "      factor = 1.0\n",
    "      top_sum = 0.0\n",
    "      bottom_sum = 0.0\n",
    "      for data in self._data_in_window:\n",
    "        value = data[key] if key in data else 0.0\n",
    "\n",
    "        top_sum += factor * value\n",
    "        bottom_sum += factor\n",
    "\n",
    "        # Update factor.\n",
    "        factor *= (1.0 - self._alpha)\n",
    "\n",
    "      smoothed_data[key] = top_sum / bottom_sum\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1678795176539,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "TEs_lgNiGv-j"
   },
   "outputs": [],
   "source": [
    "class RepetitionCounter(object):\n",
    "  \"\"\"Counts number of repetitions of given target pose class.\"\"\"\n",
    "\n",
    "  def __init__(self, class_name, enter_threshold=6, exit_threshold=4):\n",
    "    self._class_name = class_name\n",
    "\n",
    "    # If pose counter passes given threshold, then we enter the pose.\n",
    "    self._enter_threshold = enter_threshold\n",
    "    self._exit_threshold = exit_threshold\n",
    "\n",
    "    # Either we are in given pose or not.\n",
    "    self._pose_entered = False\n",
    "\n",
    "    # Number of times we exited the pose.\n",
    "    self._n_repeats = 0\n",
    "\n",
    "  @property\n",
    "  def n_repeats(self):\n",
    "    return self._n_repeats\n",
    "\n",
    "  def __call__(self, pose_classification):\n",
    "    \"\"\"Counts number of repetitions happend until given frame.\n",
    "\n",
    "    We use two thresholds. First you need to go above the higher one to enter\n",
    "    the pose, and then you need to go below the lower one to exit it. Difference\n",
    "    between the thresholds makes it stable to prediction jittering (which will\n",
    "    cause wrong counts in case of having only one threshold).\n",
    "    \n",
    "    Args:\n",
    "      pose_classification: Pose classification dictionary on current frame.\n",
    "        Sample:\n",
    "          {\n",
    "            'pushups_down': 8.3,\n",
    "            'pushups_up': 1.7,\n",
    "          }\n",
    "\n",
    "    Returns:\n",
    "      Integer counter of repetitions.\n",
    "    \"\"\"\n",
    "    # Get pose confidence.\n",
    "    pose_confidence = 0.0\n",
    "    if self._class_name in pose_classification:\n",
    "      pose_confidence = pose_classification[self._class_name]\n",
    "\n",
    "    # On the very first frame or if we were out of the pose, just check if we\n",
    "    # entered it on this frame and update the state.\n",
    "    if not self._pose_entered:\n",
    "      self._pose_entered = pose_confidence > self._enter_threshold\n",
    "      return self._n_repeats\n",
    "\n",
    "    # If we were in the pose and are exiting it, then increase the counter and\n",
    "    # update the state.\n",
    "    if pose_confidence < self._exit_threshold:\n",
    "      self._n_repeats += 1\n",
    "      self._pose_entered = False\n",
    "\n",
    "    return self._n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1678795180574,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "hgFLe1oTIgJH"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "import requests\n",
    "\n",
    "class PoseClassificationVisualizer(object):\n",
    "  \"\"\"Keeps track of classifcations for every frame and renders them.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               class_name,\n",
    "               plot_location_x=0.05,\n",
    "               plot_location_y=0.05,\n",
    "               plot_max_width=0.4,\n",
    "               plot_max_height=0.4,\n",
    "               plot_figsize=(9, 4),\n",
    "               plot_x_max=None,\n",
    "               plot_y_max=None,\n",
    "               counter_location_x=0.85,\n",
    "               counter_location_y=0.05,\n",
    "               counter_font_path='https://github.com/googlefonts/roboto/blob/main/src/hinted/Roboto-Regular.ttf?raw=true',\n",
    "               counter_font_color='red',\n",
    "               counter_font_size=0.15):\n",
    "    self._class_name = class_name\n",
    "    self._plot_location_x = plot_location_x\n",
    "    self._plot_location_y = plot_location_y\n",
    "    self._plot_max_width = plot_max_width\n",
    "    self._plot_max_height = plot_max_height\n",
    "    self._plot_figsize = plot_figsize\n",
    "    self._plot_x_max = plot_x_max\n",
    "    self._plot_y_max = plot_y_max\n",
    "    self._counter_location_x = counter_location_x\n",
    "    self._counter_location_y = counter_location_y\n",
    "    self._counter_font_path = counter_font_path\n",
    "    self._counter_font_color = counter_font_color\n",
    "    self._counter_font_size = counter_font_size\n",
    "\n",
    "    self._counter_font = None\n",
    "\n",
    "    self._pose_classification_history = []\n",
    "    self._pose_classification_filtered_history = []\n",
    "\n",
    "  def __call__(self,\n",
    "               frame,\n",
    "               pose_classification,\n",
    "               pose_classification_filtered,\n",
    "               repetitions_count):\n",
    "    \"\"\"Renders pose classifcation and counter until given frame.\"\"\"\n",
    "    # Extend classification history.\n",
    "    self._pose_classification_history.append(pose_classification)\n",
    "    self._pose_classification_filtered_history.append(pose_classification_filtered)\n",
    "\n",
    "    # Output frame with classification plot and counter.\n",
    "    output_img = Image.fromarray(frame)\n",
    "\n",
    "    output_width = output_img.size[0]\n",
    "    output_height = output_img.size[1]\n",
    "\n",
    "    # Draw the plot.\n",
    "    img = self._plot_classification_history(output_width, output_height)\n",
    "    img.thumbnail((int(output_width * self._plot_max_width),\n",
    "                   int(output_height * self._plot_max_height)),\n",
    "                  Image.ANTIALIAS)\n",
    "    output_img.paste(img,\n",
    "                     (int(output_width * self._plot_location_x),\n",
    "                      int(output_height * self._plot_location_y)))\n",
    "\n",
    "    # Draw the count.\n",
    "    output_img_draw = ImageDraw.Draw(output_img)\n",
    "    if self._counter_font is None:\n",
    "      font_size = int(output_height * self._counter_font_size)\n",
    "      font_request = requests.get(self._counter_font_path, allow_redirects=True)\n",
    "      self._counter_font = ImageFont.truetype(io.BytesIO(font_request.content), size=font_size)\n",
    "    output_img_draw.text((output_width * self._counter_location_x,\n",
    "                          output_height * self._counter_location_y),\n",
    "                         str(repetitions_count),\n",
    "                         font=self._counter_font,\n",
    "                         fill=self._counter_font_color)\n",
    "\n",
    "    return output_img\n",
    "\n",
    "  def _plot_classification_history(self, output_width, output_height):\n",
    "    fig = plt.figure(figsize=self._plot_figsize)\n",
    "\n",
    "    for classification_history in [self._pose_classification_history,\n",
    "                                   self._pose_classification_filtered_history]:\n",
    "      y = []\n",
    "      for classification in classification_history:\n",
    "        if classification is None:\n",
    "          y.append(None)\n",
    "        elif self._class_name in classification:\n",
    "          y.append(classification[self._class_name])\n",
    "        else:\n",
    "          y.append(0)\n",
    "      plt.plot(y, linewidth=7)\n",
    "\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Classification history for `{}`'.format(self._class_name))\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if self._plot_y_max is not None:\n",
    "      plt.ylim(top=self._plot_y_max)\n",
    "    if self._plot_x_max is not None:\n",
    "      plt.xlim(right=self._plot_x_max)\n",
    "\n",
    "    # Convert plot to image.\n",
    "    buf = io.BytesIO()\n",
    "    dpi = min(\n",
    "        output_width * self._plot_max_width / float(self._plot_figsize[0]),\n",
    "        output_height * self._plot_max_height / float(self._plot_figsize[1]))\n",
    "    fig.savefig(buf, dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1678795184688,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "Tw2xYlGmAt3q"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "\n",
    "class BootstrapHelper(object):\n",
    "  \"\"\"Helps to bootstrap images and filter pose samples for classification.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               images_in_folder,\n",
    "               images_out_folder,\n",
    "               csvs_out_folder):\n",
    "    self._images_in_folder = images_in_folder\n",
    "    self._images_out_folder = images_out_folder\n",
    "    self._csvs_out_folder = csvs_out_folder\n",
    "\n",
    "    # Get list of pose classes and print image statistics.\n",
    "    self._pose_class_names = sorted([n for n in os.listdir(self._images_in_folder) if not n.startswith('.')])\n",
    "    \n",
    "  def bootstrap(self, per_pose_class_limit=None):\n",
    "    \"\"\"Bootstraps images in a given folder.\n",
    "    \n",
    "    Required image in folder (same use for image out folder):\n",
    "      pushups_up/\n",
    "        image_001.jpg\n",
    "        image_002.jpg\n",
    "        ...\n",
    "      pushups_down/\n",
    "        image_001.jpg\n",
    "        image_002.jpg\n",
    "        ...\n",
    "      ...\n",
    "\n",
    "    Produced CSVs out folder:\n",
    "      pushups_up.csv\n",
    "      pushups_down.csv\n",
    "\n",
    "    Produced CSV structure with pose 3D landmarks:\n",
    "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
    "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
    "    \"\"\"\n",
    "    # Create output folder for CVSs.\n",
    "    if not os.path.exists(self._csvs_out_folder):\n",
    "      os.makedirs(self._csvs_out_folder)\n",
    "\n",
    "    for pose_class_name in self._pose_class_names:\n",
    "      print('Bootstrapping ', pose_class_name, file=sys.stderr)\n",
    "\n",
    "      # Paths for the pose class.\n",
    "      images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
    "      csv_out_path = os.path.join(self._csvs_out_folder, pose_class_name + '.csv')\n",
    "      if not os.path.exists(images_out_folder):\n",
    "        os.makedirs(images_out_folder)\n",
    "\n",
    "      with open(csv_out_path, 'w',newline=\"\") as csv_out_file:\n",
    "        csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Get list of images.\n",
    "        image_names = sorted([n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
    "        if per_pose_class_limit is not None:\n",
    "          image_names = image_names[:per_pose_class_limit]\n",
    "\n",
    "        # Bootstrap every image.\n",
    "        for image_name in tqdm.tqdm(image_names):\n",
    "          # Load image.\n",
    "          input_frame = cv2.imread(os.path.join(images_in_folder, image_name))\n",
    "          input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "          # Initialize fresh pose tracker and run it.\n",
    "          with mp_pose.Pose() as pose_tracker:\n",
    "            result = pose_tracker.process(image=input_frame)\n",
    "            pose_landmarks = result.pose_landmarks\n",
    "\n",
    "          # Save image with pose prediction (if pose was detected).\n",
    "          output_frame = input_frame.copy()\n",
    "          if pose_landmarks is not None:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=output_frame,\n",
    "                landmark_list=pose_landmarks,\n",
    "                connections=mp_pose.POSE_CONNECTIONS)\n",
    "          output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
    "          cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)\n",
    "\n",
    "          # Save landmarks if pose was detected.\n",
    "          if pose_landmarks is not None:\n",
    "            # Get landmarks.\n",
    "            frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "            pose_landmarks = np.array(\n",
    "                [[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                 for lmk in pose_landmarks.landmark],\n",
    "                dtype=np.float32)\n",
    "            assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "            csv_out_writer.writerow([image_name] + pose_landmarks.flatten().astype(np.str_).tolist())\n",
    "\n",
    "          # Draw XZ projection and concatenate with the image.\n",
    "          projection_xz = self._draw_xz_projection(\n",
    "              output_frame=output_frame, pose_landmarks=pose_landmarks)\n",
    "          output_frame = np.concatenate((output_frame, projection_xz), axis=1)\n",
    "\n",
    "  def _draw_xz_projection(self, output_frame, pose_landmarks, r=0.5, color='red'):\n",
    "    frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "    img = Image.new('RGB', (frame_width, frame_height), color='white')\n",
    "\n",
    "    if pose_landmarks is None:\n",
    "      return np.asarray(img)\n",
    "\n",
    "    # Scale radius according to the image width.\n",
    "    r *= frame_width * 0.01\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for idx_1, idx_2 in mp_pose.POSE_CONNECTIONS:\n",
    "      # Flip Z and move hips center to the center of the image.\n",
    "      x1, y1, z1 = pose_landmarks[idx_1] * [1, 1, -1] + [0, 0, frame_height * 0.5]\n",
    "      x2, y2, z2 = pose_landmarks[idx_2] * [1, 1, -1] + [0, 0, frame_height * 0.5]\n",
    "\n",
    "      draw.ellipse([x1 - r, z1 - r, x1 + r, z1 + r], fill=color)\n",
    "      draw.ellipse([x2 - r, z2 - r, x2 + r, z2 + r], fill=color)\n",
    "      draw.line([x1, z1, x2, z2], width=int(r), fill=color)\n",
    "\n",
    "    return np.asarray(img)\n",
    "\n",
    "  def align_images_and_csvs(self, print_removed_items=False):\n",
    "    \"\"\"Makes sure that image folders and CSVs have the same sample.\n",
    "\n",
    "    Leaves only intersetion of samples in both image folders and CSVs.\n",
    "    \"\"\"\n",
    "    for pose_class_name in self._pose_class_names:\n",
    "      # Paths for the pose class.\n",
    "      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
    "      csv_out_path = os.path.join(self._csvs_out_folder, pose_class_name + '.csv')\n",
    "\n",
    "      # Read CSV into memory.\n",
    "      rows = []\n",
    "      with open(csv_out_path) as csv_out_file:\n",
    "        csv_out_reader = csv.reader(csv_out_file, delimiter=',')\n",
    "        for row in csv_out_reader:\n",
    "          rows.append(row)\n",
    "\n",
    "      # Image names left in CSV.\n",
    "      image_names_in_csv = []\n",
    "\n",
    "      # Re-write the CSV removing lines without corresponding images.\n",
    "      with open(csv_out_path, 'w',newline=\"\") as csv_out_file:\n",
    "        csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in rows:\n",
    "          image_name = row[0]\n",
    "          image_path = os.path.join(images_out_folder, image_name)\n",
    "          if os.path.exists(image_path):\n",
    "            image_names_in_csv.append(image_name)\n",
    "            csv_out_writer.writerow(row)\n",
    "          elif print_removed_items:\n",
    "            print('Removed image from CSV: ', image_path)\n",
    "\n",
    "      # Remove images without corresponding line in CSV.\n",
    "      for image_name in os.listdir(images_out_folder):\n",
    "        if image_name not in image_names_in_csv:\n",
    "          image_path = os.path.join(images_out_folder, image_name)\n",
    "          os.remove(image_path)\n",
    "          if print_removed_items:\n",
    "            print('Removed image from folder: ', image_path)\n",
    "\n",
    "  def analyze_outliers(self, outliers):\n",
    "    \"\"\"Classifies each sample agains all other to find outliers.\n",
    "    \n",
    "    If sample is classified differrrently than the original class - it sould\n",
    "    either be deleted or more similar samples should be aadded.\n",
    "    \"\"\"\n",
    "    for outlier in outliers:\n",
    "      image_path = os.path.join(self._images_out_folder, outlier.sample.class_name, outlier.sample.name)\n",
    "\n",
    "      print('Outlier')\n",
    "      print('  sample path =    ', image_path)\n",
    "      print('  sample class =   ', outlier.sample.class_name)\n",
    "      print('  detected class = ', outlier.detected_class)\n",
    "      print('  all classes =    ', outlier.all_classes)\n",
    "\n",
    "      img = cv2.imread(image_path)\n",
    "      # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "      show_image(img, figsize=(20, 20))\n",
    "\n",
    "  def remove_outliers(self, outliers):\n",
    "    \"\"\"Removes outliers from the image folders.\"\"\"\n",
    "    for outlier in outliers:\n",
    "      image_path = os.path.join(self._images_out_folder, outlier.sample.class_name, outlier.sample.name)\n",
    "      os.remove(image_path)\n",
    "\n",
    "  def print_images_in_statistics(self):\n",
    "    \"\"\"Prints statistics from the input image folder.\"\"\"\n",
    "    self._print_images_statistics(self._images_in_folder, self._pose_class_names)\n",
    "\n",
    "  def print_images_out_statistics(self):\n",
    "    \"\"\"Prints statistics from the output image folder.\"\"\"\n",
    "    self._print_images_statistics(self._images_out_folder, self._pose_class_names)\n",
    "\n",
    "  def _print_images_statistics(self, images_folder, pose_class_names):\n",
    "    print('Number of images per pose class:')\n",
    "    for pose_class_name in pose_class_names:\n",
    "      n_images = len([\n",
    "          n for n in os.listdir(os.path.join(images_folder, pose_class_name))\n",
    "          if not n.startswith('.')])\n",
    "      print('  {}: {}'.format(pose_class_name, n_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpRszzilECFz"
   },
   "source": [
    "## 训练集数据结构\n",
    "\n",
    "\n",
    "```\n",
    "jump_dataset/\n",
    "  close/\n",
    "    001.jpg\n",
    "    002.jpg\n",
    "    ...\n",
    "  open/\n",
    "    001.jpg\n",
    "    002.jpg\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1678795214569,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "5_L_IRRA66id",
    "outputId": "254747f7-866d-44cd-9540-220c5f2ce9dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'JumpCounter.ipynb',\n",
       " 'jumptest-output.mp4',\n",
       " 'jumptest.mp4',\n",
       " 'jump_csvs_out',\n",
       " 'jump_dataset',\n",
       " 'jump_images_out',\n",
       " 'jump_out_basic']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1678795224552,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "oqNELyqv7XGb"
   },
   "outputs": [],
   "source": [
    "# 训练数据集\n",
    "bootstrap_images_in_folder = 'jump_dataset'\n",
    "\n",
    "# 输出csv文件和带有标注关键点的图片\n",
    "bootstrap_images_out_folder = 'jump_images_out'\n",
    "bootstrap_csvs_out_folder = 'jump_csvs_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1678795226560,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "3jw34l6n705h"
   },
   "outputs": [],
   "source": [
    "# initalize the helper\n",
    "bootstrap_helper = BootstrapHelper(\n",
    "    images_in_folder=bootstrap_images_in_folder,\n",
    "    images_out_folder=bootstrap_images_out_folder,\n",
    "    csvs_out_folder=bootstrap_csvs_out_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1534,
     "status": "ok",
     "timestamp": 1678795230403,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "5gEc7AXc8H3X",
    "outputId": "50576e3f-1856-456a-a687-e5165204d5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  close: 724\n",
      "  open: 785\n"
     ]
    }
   ],
   "source": [
    "bootstrap_helper.print_images_in_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 389704,
     "status": "ok",
     "timestamp": 1678795706788,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "wSwgteZe8gf4",
    "outputId": "b3ab40f5-836d-4bd1-f590-524d72fa0ec2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping  close\n",
      " 41%|████████████████████████████████▌                                               | 295/724 [01:09<01:42,  4.19it/s]"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "bootstrap_helper.bootstrap(per_pose_class_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1162,
     "status": "ok",
     "timestamp": 1678795710519,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "IaArX5mk-KGA",
    "outputId": "16442b6e-61b5-4307-bcde-b9a76c3ca80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  close: 724\n",
      "  open: 791\n"
     ]
    }
   ],
   "source": [
    "# check how much image extract the feature\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1678795712813,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "5SJ7Y6Tp-XHe",
    "outputId": "2ee34e78-abe6-424f-ec28-ea9583e23619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  close: 701\n",
      "  open: 790\n"
     ]
    }
   ],
   "source": [
    "# Align CSVs with filtered images\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1155,
     "status": "ok",
     "timestamp": 1678795716339,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "BZx3vCugKusv",
    "outputId": "3d7f01fa-93ed-4ab6-ddf2-f8249bb1c6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  close: 701\n",
      "  open: 790\n"
     ]
    }
   ],
   "source": [
    "# After initial bootstrapping images without detected poses were still saved in\n",
    "# the folderd (but not in the CSVs) for debug purpose. Let's remove them.\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62071,
     "status": "ok",
     "timestamp": 1678795840380,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "lkwUIZXc-nqW",
    "outputId": "d1795e74-5be2-4357-825f-526b594daac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers:  6\n"
     ]
    }
   ],
   "source": [
    "# transforms pose landmarks into embedding\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# classifies give pose against database of poses \n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=bootstrap_csvs_out_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "outliers = pose_classifier.find_pose_sample_outliers()\n",
    "print(\"Number of outliers: \",len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1l0wrueEOZMxEFGzM4PIc3lFcAI5ZI9eC"
    },
    "executionInfo": {
     "elapsed": 11095,
     "status": "ok",
     "timestamp": 1678795867176,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "iDhjzZPCATN_",
    "outputId": "78c817a0-bd1a-4305-9fdc-c28f5ce753ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the Anomalies\n",
    "bootstrap_helper.analyze_outliers(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1678795880969,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "JD3R-dbJCWi4"
   },
   "outputs": [],
   "source": [
    "# remove the anomalies\n",
    "bootstrap_helper.remove_outliers(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2481,
     "status": "ok",
     "timestamp": 1678795886751,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "OgM2nYseCe4J",
    "outputId": "5811c884-0520-4ff4-f29f-389b886c5ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  close: 695\n",
      "  open: 790\n"
     ]
    }
   ],
   "source": [
    "# Rearranging dichotomous data\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1678795896532,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "NvH52I5wGCVV",
    "outputId": "63cda378-1b0a-4acb-e86c-8fa55df9b00b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ef69a4b6-92e0-4e4a-a25d-4052b5957084\", \"jump_csvs_out_basic.csv\", 1448469)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate csv file\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def dump_for_the_app():\n",
    "  pose_samples_folder = 'jump_csvs_out'\n",
    "  pose_samples_csv_path = 'jump_csvs_out_basic.csv'\n",
    "  file_extension = 'csv'\n",
    "  file_separator = ','\n",
    "\n",
    "  # Each file in the folder represents one pose class\n",
    "  file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
    "\n",
    "  with open(pose_samples_csv_path,'w') as csv_out:\n",
    "    csv_out_writer = csv.writer(csv_out,delimiter=file_separator,quoting=csv.QUOTE_MINIMAL)\n",
    "    for file_name in file_names:\n",
    "      class_name = file_name[:-(len(file_extension)+1)]\n",
    "      with open(os.path.join(pose_samples_folder,file_name)) as csv_in:\n",
    "        csv_in_reader = csv.reader(csv_in,delimiter=file_separator)\n",
    "        for row in csv_in_reader:\n",
    "          row.insert(1,class_name)\n",
    "          csv_out_writer.writerow(row)\n",
    "dump_for_the_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xdu3DGbRNzDF"
   },
   "source": [
    "### 对视频做出处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1678795914651,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "2SFb2JhqNxTH"
   },
   "outputs": [],
   "source": [
    "# upload video file and output the processed file\n",
    "video_path = 'jumptest.mp4'\n",
    "class_name = 'open'\n",
    "out_video_path = 'jumptest-output.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1678795916956,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "J7L-_0onOJcd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "# get some video parameters to generate output video with classfication \n",
    "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1678795919204,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "yr-3yDvGTg0W"
   },
   "outputs": [],
   "source": [
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "# building classifier to output CSVs\n",
    "pose_samples_folder = 'jump_csvs_out'\n",
    "\n",
    "# initialize tracker\n",
    "pose_tracker = mp_pose.Pose(upper_body_only=False)\n",
    "\n",
    "# initialize embedder\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# initialize classifier\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=pose_samples_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "# initialize EMA smoothing\n",
    "pose_classification_filter = EMADictSmoothing(window_size=10,alpha=0.2)\n",
    "\n",
    "# two thresholds for the specified action\n",
    "repetition_counter = RepetitionCounter(class_name=class_name,enter_threshold=6,exit_threshold=4)\n",
    "\n",
    "# initialize renderer\n",
    "pose_classification_visualizer = PoseClassificationVisualizer(class_name=class_name,plot_x_max=video_n_frames,plot_y_max=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1ppijwLbLIZSlu6K6Q5ehgM_tjauU9VVg"
    },
    "executionInfo": {
     "elapsed": 99736,
     "status": "ok",
     "timestamp": 1678796021741,
     "user": {
      "displayName": "张祎阳",
      "userId": "06715259925771924263"
     },
     "user_tz": -480
    },
    "id": "Eq3r62VYVhvT",
    "outputId": "c1474867-8384-42f4-8d36-d479ac2767c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "# Open output video\n",
    "out_video = cv2.VideoWriter(out_video_path,cv2.VideoWriter_fourcc(*'mp4v'),video_fps,(video_width,video_height))\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "with tqdm.tqdm(total=video_n_frames,position=0,leave=True) as pbar:\n",
    "  while True:\n",
    "    # get next frame of the video\n",
    "    success,input_frame = video_cap.read()\n",
    "    if not success:\n",
    "      break\n",
    "    # Run pose tracker\n",
    "    input_frame = cv2.cvtColor(input_frame,cv2.COLOR_BGR2RGB)\n",
    "    result = pose_tracker.process(image=input_frame)\n",
    "    pose_landmarks = result.pose_landmarks\n",
    "\n",
    "    # draw pose prediction\n",
    "    output_frame = input_frame.copy()\n",
    "    if pose_landmarks is not None:\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=output_frame,\n",
    "          landmark_list=pose_landmarks,\n",
    "          connections=mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    if pose_landmarks is not None:\n",
    "      # get landmarks\n",
    "      frame_height, frame_width = output_frame.shape[0],output_frame.shape[1]\n",
    "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width] for lmk in pose_landmarks.landmark],dtype=np.float32)\n",
    "      assert pose_landmarks.shape == (33,3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "      # classify the pose on the current frame\n",
    "      pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "      # smooth classification using EMA\n",
    "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "      # count repetitions \n",
    "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "    else:\n",
    "      # No pose -> no classfication on current frame\n",
    "      pose_classification = None\n",
    "\n",
    "      # smoothing for future frames\n",
    "      pose_classification_filtered = pose_classification_filter(dict())\n",
    "      pose_classification_filtered = None\n",
    "\n",
    "      # take the latest repetitions count\n",
    "      repetitions_count = repetition_counter.n_repeats\n",
    "    \n",
    "    # draw classification plot and repetition counter\n",
    "    output_frame = pose_classification_visualizer(\n",
    "        frame=output_frame,\n",
    "        pose_classification=pose_classification,\n",
    "        pose_classification_filtered=pose_classification_filtered,\n",
    "        repetitions_count=repetitions_count)\n",
    "    \n",
    "    # save the output frame\n",
    "    out_video.write(cv2.cvtColor(np.array(output_frame),cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # show intermediate frames of the video to track progress\n",
    "    if frame_idx % 50 == 0:\n",
    "      show_image(output_frame)\n",
    "    \n",
    "    frame_idx += 1\n",
    "    pbar.update()\n",
    "\n",
    "# close output video\n",
    "out_video.release()\n",
    "\n",
    "# releease mediaPipe resources \n",
    "pose_tracker.close()\n",
    "\n",
    "# show the last frame of the video\n",
    "if output_frame is not None:\n",
    "  show_image(output_frame)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fecea6e50ddc6ee70afed96ec4f07138b329af307f96b4713a57e17073b40d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
